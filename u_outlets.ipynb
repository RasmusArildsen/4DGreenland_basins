{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce03e8df-4f98-42c7-9039-986b9570efbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ r.in.gdal → dem\n",
      "GRASS 8.4.1 (2025)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, sys, tempfile, getpass\n",
    "from pathlib import Path\n",
    "QGIS_PREFIX       = r\"C:\\Users\\B464518\\AppData\\Local\\Programs\\OSGeo4W\"   # QGIS install folder\n",
    "DEM               = r\"C:\\Users\\B464518\\drainage\\raster\\arcticDEM_500m_ice_sheet.tif\"   # DEM\n",
    "icesheet          = r\"C:\\Users\\B464518\\drainage\\vector\\02_PROMICE-2022-IceMask-polygon.gpkg\"\n",
    "OUT               = r\"C:\\Users\\B464518\\drainage\\output\\moede_mandag\" \n",
    "STREAM_THRESHOLD  = 200          # Ved subglaciale bassiner: anden opløsning af dem, hæv threshold til tilsvarende m2.\n",
    "\n",
    "# ------------------\n",
    "    # Importér+definér DEM og maske og set region\n",
    "GISBASE = fr\"{QGIS_PREFIX}\\apps\\grass\\grass84\"\n",
    "os.environ[\"GISBASE\"]      = GISBASE\n",
    "os.environ[\"GRASS_PYTHON\"] = sys.executable\n",
    "os.environ[\"PROJ_LIB\"]     = fr\"{QGIS_PREFIX}\\share\\proj\"\n",
    "os.environ[\"GDAL_DATA\"]    = fr\"{QGIS_PREFIX}\\share\\gdal\"\n",
    "os.environ[\"PATH\"] = os.pathsep.join([\n",
    "    fr\"{GISBASE}\\bin\", fr\"{GISBASE}\\extrabin\",\n",
    "    fr\"{QGIS_PREFIX}\\bin\", fr\"{QGIS_PREFIX}\\apps\\Qt5\\bin\",\n",
    "    os.environ[\"PATH\"],\n",
    "])\n",
    "sys.path.insert(0, fr\"{GISBASE}\\etc\\python\")\n",
    "\n",
    "import grass.script as gs\n",
    "import grass.script.setup as gsetup\n",
    "\n",
    "Path(OUT).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def start_grass_from_raster(raster_path, location=\"dem_loc\", mapset=\"PERMANENT\"):\n",
    "    \"\"\"Create/ensure LOCATION from DEM, init session by MAPSET path.\"\"\"\n",
    "    gisdbase = os.path.join(tempfile.gettempdir(), f\"grassdata_{getpass.getuser()}\")\n",
    "    Path(gisdbase).mkdir(parents=True, exist_ok=True)\n",
    "    if not (Path(gisdbase)/location).exists():\n",
    "        gs.core.create_location(dbase=gisdbase, location=location, filename=raster_path, overwrite=True)\n",
    "    gsetup.init(path=str(Path(gisdbase)/location/mapset))\n",
    "    return gisdbase, location, mapset\n",
    "    \n",
    "def import_dem_native(input_path, out_name=\"dem\"):\n",
    "    \"\"\"Import or clone to a true GRASS raster (some modules dislike r.external).\"\"\"\n",
    "    try:\n",
    "        gs.run_command(\"r.in.gdal\", input=input_path.replace(\"\\\\\",\"/\"), output=out_name, flags=\"o\", overwrite=True)\n",
    "        print(f\"✓ r.in.gdal → {out_name}\")\n",
    "    except Exception:\n",
    "        gs.run_command(\"r.external\", input=input_path.replace(\"\\\\\",\"/\"), output=f\"{out_name}_ext\", flags=\"o\", overwrite=True)\n",
    "        gs.run_command(\"g.region\", raster=f\"{out_name}_ext\")\n",
    "        gs.mapcalc(f\"{out_name} = {out_name}_ext * 1.0\", overwrite=True)\n",
    "        print(f\"✓ r.external + clone → {out_name}\")\n",
    "    gs.run_command(\"g.region\", raster=out_name)\n",
    "\n",
    "\n",
    "def have(module_name):\n",
    "    from shutil import which\n",
    "    return which(module_name) is not None\n",
    "\n",
    "# === 1) Start GRASS session and import DEM ===\n",
    "GISDBASE, LOCATION, MAPSET = start_grass_from_raster(DEM)\n",
    "import_dem_native(DEM, out_name=\"dem\")  # creates native GRASS raster 'dem'\n",
    "##GRASS check before starting\n",
    "version_info = gs.read_command(\"g.version\")\n",
    "print(version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a24ad37-4dd2-44ae-84ca-29cd42de0c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------- Maske ----------#\n",
    "gs.run_command('v.import', input=icesheet.replace(\"\\\\\",\"/\"), output='icesheet_mask', overwrite=True)\n",
    "gs.run_command('r.mask',\n",
    "                vector='icesheet_mask',\n",
    "                maskcats=None,\n",
    "                overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3dd0ee-955a-49bf-8f7a-ba7d2971d0a2",
   "metadata": {},
   "source": [
    "## Udregning af hydraulisk potentiale (ikke relevant for supraglacialt) ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a565022-8d50-48e1-a58d-77e136a9d40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Input\n",
    "ice_file = r\"C:\\Users\\B464518\\drainage\\raster\\bedmachine_surface.tif\"\n",
    "bed_file = r\"C:\\Users\\B464518\\drainage\\raster\\bedmachine_bed.tif\"\n",
    "gs.run_command('r.import', input=bed_file, output='bed_raster', overwrite=True)\n",
    "gs.run_command('r.import', input=ice_file, output='ice_raster', overwrite=True)\n",
    "\n",
    "# Output\n",
    "output_file = r\"C:\\Users\\B464518\\drainage\\raster\\hydraulic_potential.tif\"\n",
    "\n",
    "#----------smoothing (kan udlades)-----------#\n",
    "gs.run_command('r.neighbors',\n",
    "                input=\"ice_raster\", #skift til 'dem' ved overfladebassiner\n",
    "                output='dem_smoothed',\n",
    "                method='average',\n",
    "                size=15,\n",
    "                overwrite=True)\n",
    "\n",
    "# variable\n",
    "rho_w = 1000    # Densitet vand (kg/m³)\n",
    "rho_i = 917     # Densitet is (kg/m³)\n",
    "g = 9.81        # tyngdeacceleration (m/s²)\n",
    "\n",
    "# Udregn hydr. pot (indenfor masken)\n",
    "gs.mapcalc(f\"hydraulic_potential = (({rho_w} * {g} * bed_raster) + ({rho_i} * {g} * (dem_smoothed-bed_raster)))/1000000\", overwrite=True)\n",
    "\n",
    "# Eksporter\n",
    "gs.run_command('r.out.gdal', input='hydraulic_potential', output=output_file, format='GTiff', type ='Float64', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d38e94-369a-4319-98f8-3387ee733c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CALCULATE STREAMS AND ACCUMULATION MAP ##\n",
    "\n",
    "# create accumulation map explicitly\n",
    "gs.run_command('r.watershed', \n",
    "               elevation='dem', \n",
    "               accumulation='accum', \n",
    "               overwrite=True) \n",
    "\n",
    "\n",
    "#----------Fill sinks + flow direction----------#\n",
    "gs.run_command('r.fill.dir',\n",
    "                input='dem',\n",
    "                output='dem_filled',\n",
    "                direction='flow_dir',\n",
    "                overwrite=True)\n",
    "\n",
    "#----------Extract streams----------# \n",
    "gs.run_command('r.stream.extract', \n",
    "               elevation='dem_filled', \n",
    "               direction='flow_dir', \n",
    "               accumulation=\"accum\", \n",
    "               threshold=STREAM_THRESHOLD, d8cut=0, mexp=0.0, #dette er de eneste variable, der eksplicit defineres i dette script. \n",
    "               stream_raster='streams', \n",
    "               stream_vector='streams_vect', overwrite=True) \n",
    "\n",
    "## OUTLET DEFINITIONS ##\n",
    "\n",
    "#----------Define stream orders to get the outlet streams (where next_stream=-1)----------# \n",
    "gs.run_command('r.stream.order', \n",
    "               stream_rast='streams', \n",
    "               direction='flow_dir', \n",
    "               elevation='dem_filled', \n",
    "               accumulation ='accum', \n",
    "               strahler='strahler', \n",
    "               stream_vect='streams_vect',\n",
    "               overwrite=True) \n",
    "\n",
    "#----------Extract only streams with no next_stream ----------# \n",
    "gs.run_command('v.extract', \n",
    "               input='streams_vect', \n",
    "               output='outlet_segment', \n",
    "               where=\"next_stream = -1\", \n",
    "               overwrite=True) \n",
    "#----------Get the end points of those streams ----------# \n",
    "gs.run_command('v.to.points', \n",
    "               input='outlet_segment', \n",
    "               output='outlet_point', \n",
    "               use='end', # downstream end \n",
    "               flags='r', \n",
    "               overwrite=True)\n",
    "#----------export to gpkg to work w geopandas ----------# \n",
    "gs.run_command('v.out.ogr', input ='outlet_point', format ='GPKG', output = 'outlets.gpkg', overwrite=True)\n",
    "\n",
    "import geopandas as gpd\n",
    "# 1. Read the GeoPackage\n",
    "gdf = gpd.read_file(\"outlets.gpkg\")\n",
    "\n",
    "\n",
    "# 2. Add a new column with running numbers starting at 1\n",
    "gdf['new_id'] = range(1, len(gdf) + 1)\n",
    "\n",
    "# 2. Keep only one point per stream (lowest id is apparently always the start of the stream, and the highest is the end of each stream)\n",
    "gdf_clean = gdf.loc[gdf.groupby('stream')['new_id'].idxmax()]\n",
    "\n",
    "# 3. Save cleaned outlets back to a new GeoPackage\n",
    "gdf_clean.to_file(fr\"{OUT}\\outlets_clean.gpkg\", driver=\"GPKG\")\n",
    "\n",
    "gs.run_command('v.import', input='outlets_clean.gpkg', output='outlet_point_clean', overwrite=True)\n",
    "\n",
    "## BASINS DELINEATION ##\n",
    "\n",
    "# Delineate basins using points. If you would rather do it without the whole outlet extraction, use the streams raster as input instead. the r.stream.basins documentation describes it very nicely.\n",
    "gs.run_command('r.stream.basins',\n",
    "               direction='flow_dir',\n",
    "               basins='basins',\n",
    "               points='outlet_point_clean',\n",
    "               overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9e08e1c-d88e-4c9e-8eaf-af51954628aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------eksporter----------#\n",
    "for name, fn in [\n",
    "    (\"dem\", fr\"{OUT}\\dem_arctic_200.tif\"),\n",
    "    (\"accum\", fr\"{OUT}\\accum__arctic_200.tif\"),\n",
    "    (\"flow_dir\", fr\"{OUT}\\flowdir_arctic_200.tif\"),\n",
    "    (\"streams\", fr\"{OUT}\\streams_arctic_200.tif\"),\n",
    "    (\"basins\", fr\"{OUT}\\basins_arctic_200.tif\"),\n",
    "]:\n",
    "    gs.run_command(\"r.out.gdal\", input=name, output=fn.replace(\"\\\\\",\"/\"), format=\"GTiff\",\n",
    "                   createopt=\"COMPRESS=LZW\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575c180e-0259-425d-b706-dd65ca497220",
   "metadata": {},
   "source": [
    "## Script from Rasmus to merge basins that are too small and assign cells with no basin to nearest basin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c0bc080-05fa-45fc-ba87-627c391e6beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Merge threshold: 5 km² ≈ 20 cells at 500 m\n",
      "→ Exclave cleanup threshold: 10 km² ≈ 40 cells\n",
      "→ Big basins: 117  |  Small basins: 250\n",
      "\n",
      "→ Exclave pass 1\n",
      "  small clumps found: 1094\n",
      "\n",
      "→ Exclave pass 2\n",
      "  small clumps found: 979\n",
      "\n",
      "→ Exclave pass 3\n",
      "  small clumps found: 963\n",
      "\n",
      "→ Exclave pass 4\n",
      "  small clumps found: 956\n",
      "✅ Done → C:\\Users\\B464518\\drainage\\output\\moede_mandag\\basins_merged_arctic_200.tif\n"
     ]
    }
   ],
   "source": [
    "def _parse_stat_value(tok: str):\n",
    "    \"\"\"Parse r.stats category token which can be '13', '13.0', or '13-13'.\"\"\"\n",
    "    tok = tok.strip()\n",
    "    if tok in (\"\", \"*\"):\n",
    "        return None\n",
    "    if \"-\" in tok:                  # e.g., '13-13'\n",
    "        tok = tok.split(\"-\", 1)[0]  # take the left edge\n",
    "    try:\n",
    "        return int(round(float(tok)))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "dem = 'hydraulic_potential'\n",
    "MIN_BASIN_SIZE_KM2 = 5      # set your threshold (e.g., 10000 km²)\n",
    "res = 500                      # DEM working resolution (m)\n",
    "\n",
    "# Optional: exclave cleanup after merge. Set 0 to disable, or a smaller area than MIN_BASIN_SIZE_KM2.\n",
    "EXCLAVE_MAX_KM2   = 10         # e.g., 50 to only remove tiny specks; 0 disables\n",
    "MAX_EXCLAVE_ITERS = 4           # safety cap\n",
    "# ===============================================\n",
    "\n",
    "# Force int labels, mask to DEM extent\n",
    "gs.mapcalc(\"basins0 = int(basins)\", overwrite=True)\n",
    "gs.mapcalc(\"basins0 = if(isnull(dem), null(), basins0)\", overwrite=True)\n",
    "\n",
    "# Thresholds in cells\n",
    "cell_area_km2 = (res*res)/1e6\n",
    "min_cells     = int(round(MIN_BASIN_SIZE_KM2 / cell_area_km2))\n",
    "exclave_cells = int(round(EXCLAVE_MAX_KM2 / cell_area_km2)) if EXCLAVE_MAX_KM2 > 0 else 0\n",
    "print(f\"→ Merge threshold: {MIN_BASIN_SIZE_KM2} km² ≈ {min_cells} cells at {res} m\")\n",
    "if exclave_cells > 0:\n",
    "    print(f\"→ Exclave cleanup threshold: {EXCLAVE_MAX_KM2} km² ≈ {exclave_cells} cells\")\n",
    "\n",
    "# ---------- Split big vs small by basin area ----------\n",
    "stats = gs.read_command(\"r.stats\", input=\"basins0\", flags=\"cn\", separator=\",\").strip().splitlines()\n",
    "if not stats:\n",
    "    raise RuntimeError(\"No basins found within DEM extent.\")\n",
    "sizes = {int(cat): int(n) for cat, n in (ln.split(\",\") for ln in stats)}\n",
    "big_ids   = {cat for cat, n in sizes.items() if n >= min_cells}\n",
    "small_ids = sorted(set(sizes) - big_ids)\n",
    "\n",
    "if not big_ids:\n",
    "    largest = max(sizes.items(), key=lambda kv: kv[1])[0]\n",
    "    big_ids = {largest}\n",
    "    small_ids = sorted(set(sizes) - big_ids)\n",
    "    print(f\"ℹ️ All basins < threshold; seeding with largest basin {largest}\")\n",
    "\n",
    "print(f\"→ Big basins: {len(big_ids)}  |  Small basins: {len(small_ids)}\")\n",
    "\n",
    "# If nothing to merge → carry forward\n",
    "if not small_ids:\n",
    "    gs.mapcalc(\"basins_after_merge = basins0\", overwrite=True)\n",
    "else:\n",
    "    # 1) big-only raster (others NULL)\n",
    "    rules_big = Path(OUT)/\"big_reclass.txt\"\n",
    "    with open(rules_big, \"w\", encoding=\"utf-8\") as f:\n",
    "        for cat in sizes:\n",
    "            f.write(f\"{cat} = {cat}\\n\" if cat in big_ids else f\"{cat} = NULL\\n\")\n",
    "    gs.run_command(\"r.reclass\", input=\"basins0\", output=\"big_only\",\n",
    "                   rules=str(rules_big), overwrite=True)\n",
    "\n",
    "    # 2) nearest big ID per cell\n",
    "    gs.run_command(\"r.grow.distance\", input=\"big_only\", value=\"nearest_big_id\",\n",
    "                   flags=\"m\", overwrite=True)\n",
    "\n",
    "    # 3) For each small basin, pick ONE big neighbour by majority (mode) of nearest_big_id *within that basin*\n",
    "    #    Build reclass rules: small_id = chosen_big_id; big_id = big_id (identity)\n",
    "    rules_path = Path(OUT) / \"whole_basin_reclass.txt\"\n",
    "    with open(rules_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        # identity for bigs\n",
    "        for bid in big_ids:\n",
    "            f.write(f\"{bid} = {bid}\\n\")\n",
    "    \n",
    "        # choose one neighbour per small basin by majority vote\n",
    "        for sid in small_ids:\n",
    "            masked = f\"nbid_{sid}\"\n",
    "            gs.mapcalc(f\"{masked} = if(basins0 == {sid}, nearest_big_id, null())\", overwrite=True)\n",
    "    \n",
    "            # Exclude NULLs with 'N' flag; request category and count\n",
    "            lines = gs.read_command(\"r.stats\", input=masked, flags=\"cnN\", separator=\",\").strip().splitlines()\n",
    "            if not lines:\n",
    "                # degenerate: no vote — keep itself\n",
    "                f.write(f\"{sid} = {sid}\\n\")\n",
    "                continue\n",
    "    \n",
    "            counts = []\n",
    "            for ln in lines:\n",
    "                val_s, cnt_s = ln.split(\",\")\n",
    "                val = _parse_stat_value(val_s)\n",
    "                if val is None:\n",
    "                    continue\n",
    "                try:\n",
    "                    cnt = int(cnt_s)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                counts.append((val, cnt))\n",
    "    \n",
    "            if not counts:\n",
    "                # still nothing usable; keep itself\n",
    "                f.write(f\"{sid} = {sid}\\n\")\n",
    "                continue\n",
    "    \n",
    "            chosen = max(counts, key=lambda vc: vc[1])[0]\n",
    "            f.write(f\"{sid} = {chosen}\\n\")\n",
    "\n",
    "\n",
    "    # 4) Reclass whole basins in one shot (guarantees NO splitting)\n",
    "    gs.run_command(\"r.reclass\", input=\"basins0\", output=\"basins_after_merge\",\n",
    "                   rules=str(rules_path), overwrite=True)\n",
    "\n",
    "# ---------- Full DEM coverage (fill any NULLs) ----------\n",
    "gs.mapcalc(\"basins_after_merge = if(isnull(dem), null(), basins_after_merge)\", overwrite=True)\n",
    "gs.run_command(\"r.grow.distance\", input=\"basins_after_merge\", value=\"fill_from\",\n",
    "               flags=\"m\", overwrite=True)\n",
    "gs.mapcalc(\"basins_filled = if(isnull(basins_after_merge) && !isnull(dem), fill_from, basins_after_merge)\",\n",
    "           overwrite=True)\n",
    "\n",
    "# ---------- Optional: Iterative EXCLAVE cleanup (small disconnected patches) ----------\n",
    "current = \"basins_filled\"\n",
    "if exclave_cells > 0:\n",
    "    for it in range(1, MAX_EXCLAVE_ITERS+1):\n",
    "        print(f\"\\n→ Exclave pass {it}\")\n",
    "        gs.run_command(\"r.clump\", input=current, output=\"clumps\", overwrite=True)\n",
    "        cstats = gs.read_command(\"r.stats\", input=\"clumps\", flags=\"cn\",\n",
    "                                 separator=\",\").strip().splitlines()\n",
    "        if not cstats:\n",
    "            print(\"  (no clumps?)\")\n",
    "            break\n",
    "        small_clump_ids = [int(cid) for cid, n in (ln.split(\",\") for ln in cstats)\n",
    "                           if int(n) < exclave_cells]\n",
    "        print(f\"  small clumps found: {len(small_clump_ids)}\")\n",
    "\n",
    "        if not small_clump_ids:\n",
    "            print(\"  ✓ no exclaves under threshold remain; stopping\")\n",
    "            break\n",
    "\n",
    "        expr = \" || \".join([f\"clumps == {cid}\" for cid in small_clump_ids])\n",
    "        gs.mapcalc(f\"smallmask = if({expr}, 1, null())\", overwrite=True)\n",
    "        gs.mapcalc(f\"{current}_nulled = if(!isnull(smallmask), null(), {current})\", overwrite=True)\n",
    "\n",
    "        gs.run_command(\"r.grow.distance\", input=f\"{current}_nulled\", value=\"refill\",\n",
    "                       flags=\"m\", overwrite=True)\n",
    "        gs.mapcalc(f\"{current} = if(isnull({current}_nulled) && !isnull(dem), refill, {current}_nulled)\",\n",
    "                   overwrite=True)\n",
    "else:\n",
    "    print(\"→ Exclave cleanup disabled (EXCLAVE_MAX_KM2=0).\")\n",
    "\n",
    "# ---------- Final export ----------\n",
    "final_map = current\n",
    "gs.mapcalc(\"basins_merged_final = int({})\".format(final_map), overwrite=True)\n",
    "gs.run_command(\"r.out.gdal\",\n",
    "               input=\"basins_merged_final\",\n",
    "               output=str(Path(OUT)/\"basins_merged_arctic_200.tif\"),\n",
    "               format=\"GTiff\", createopt=\"COMPRESS=LZW\", overwrite=True)\n",
    "\n",
    "print(\"✅ Done →\", Path(OUT)/\"basins_merged_arctic_200.tif\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e3b1fb-9736-40a6-8b4c-60d12ec852f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
